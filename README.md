# MLops-Project
End to End MLops project

ğŸš— End-to-End MLOps Pipeline
This repository showcases a production-grade MLOps pipeline built for vehicle insurance prediction using FastAPI, MongoDB, AWS (S3, EC2, ECR), and GitHub Actions CI/CD. It covers the entire lifecycle: from data ingestion, validation, transformation, model training to deployment using Docker and AWS cloud infrastructure.

ğŸ“ Project Structure (Workflow)
arduino
Copy
Edit
â”œâ”€â”€ constants/
â”œâ”€â”€ config/
â”œâ”€â”€ entity/
â”‚   â”œâ”€â”€ config_entity.py
â”‚   â”œâ”€â”€ artifact_entity.py
â”‚   â””â”€â”€ s3_estimator.py
â”œâ”€â”€ components/
â”œâ”€â”€ pipeline/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_access/
â”‚   â”œâ”€â”€ configuration/
â”‚   â””â”€â”€ aws_storage/
â”œâ”€â”€ templates/
â”œâ”€â”€ static/
â”œâ”€â”€ app.py / demo.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ setup.py
â”œâ”€â”€ pyproject.toml
ğŸš€ Workflow Overview
âœ… Project Initialization
Create template: Run template.py to scaffold the project.

Package Management: Set up setup.py and pyproject.toml for local package imports.

Virtual Environment:

bash
Copy
Edit
conda create -n vehicle python=3.10 -y
conda activate vehicle
pip install -r requirements.txt
âœ… MongoDB Setup
Use MongoDB Atlas to host datasets.

Fetch data via mongoDB_demo.ipynb.

Store data using PyMongo in key-value format.

âœ… Logging, Exceptions, and EDA
Modular logging and exception handling setup.

notebooks/ folder includes EDA and Feature Engineering.

ğŸ”„ ML Pipeline Components
ğŸ“¥ 1. Data Ingestion
Fetch data from MongoDB Atlas

Store raw data and metadata in local artifact/ folder

ğŸ§¹ 2. Data Validation
Schema-driven validation using schema.yaml

Logs and reports stored for transparency

ğŸ”§ 3. Data Transformation
Scaling, encoding, and train-test split

Handled via estimator.py and pipelines

ğŸ¤– 4. Model Trainer
Train multiple ML models

Evaluate using metrics (RÂ², MAE, RMSE)

âœ… AWS Setup & Model Registry
ğŸ“¦ S3 Integration
Trained models pushed to AWS S3 bucket

Set AWS credentials in environment or .env:

bash
Copy
Edit
export AWS_ACCESS_KEY_ID="..."
export AWS_SECRET_ACCESS_KEY="..."
ğŸ” Model Evaluation
Compare new model against baseline using delta threshold

Only push to registry if performance improves

ğŸš› Model Pusher
Push best model to model registry folder in S3

ğŸŒ App Deployment with CI/CD (FastAPI)
ğŸ³ Docker Setup
Build & tag Docker image

Push to AWS ECR

âš™ï¸ GitHub Actions CI/CD
Auto-deploy on commit via .github/workflows/aws.yaml

Secrets stored securely in GitHub Secrets

â˜ï¸ EC2 + Self-Hosted Runner
EC2 instance runs app using port 5080

Runner connected to GitHub for deployment

ğŸ“Œ Accessing the App
Launch EC2 instance (T2 Medium / Ubuntu)

Install Docker & set up GitHub self-hosted runner

Open browser:

url
Copy
Edit
http://<EC2-Public-IP>:5000
Use /train route to trigger model training.

ğŸ”’ Security Notes
Do not commit .env or credential files

Add artifact/, __pycache__/, and .env to .gitignore

ğŸ“Š Tools & Technologies
Area	Stack Used
Web Framework	FastAPI, Jinja2
Database	MongoDB Atlas
ML & Pipeline	scikit-learn, Pandas, PyYAML
Cloud Infrastructure	AWS S3, EC2, ECR, IAM
DevOps	Docker, GitHub Actions, Self-hosted
Logging & Exception	Custom modules



ğŸ‘¨â€ğŸ’» Author
Udaybhan Singh Rana
Product + ML Engineering | MLOps | AWS | FastAPI | GitHub Actions